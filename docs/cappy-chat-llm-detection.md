# ğŸ¤– Cappy Chat - Sistema de DetecÃ§Ã£o DinÃ¢mica de LLMs

## ğŸ“‹ VisÃ£o Geral

O Cappy Chat agora detecta automaticamente os modelos de linguagem (LLMs) disponÃ­veis no seu ambiente VS Code, permitindo que vocÃª escolha qual agente usar para suas conversas.

## ğŸ” Fontes de DetecÃ§Ã£o

### 1. **VS Code Language Model API** (Oficial)
```typescript
vscode.lm.selectChatModels()
```
- Detecta modelos registrados via API oficial do VS Code
- Suporta extensÃµes que implementam a interface `vscode.lm`
- Inclui vendor/provider/family metadata

### 2. **GitHub Copilot**
- Detecta se `github.copilot` ou `github.copilot-chat` estÃ£o instalados e ativos
- Oferece:
  - GPT-4 (modelo mais capaz)
  - GPT-3.5 Turbo (rÃ¡pido e eficiente)

### 3. **Modelos Personalizados** (ConfiguraÃ§Ã£o do UsuÃ¡rio)
```json
{
  "cappy.chat.customModels": [
    {
      "id": "my-gpt4",
      "name": "My GPT-4 Instance",
      "provider": "openai",
      "apiKey": "sk-...",
      "endpoint": "https://api.openai.com/v1",
      "available": true
    }
  ]
}
```

### 4. **Ollama** (Modelos Locais)
- Detecta se a extensÃ£o `continue.continue` estÃ¡ instalada
- LÃª modelos configurados em `cappy.chat.ollama.models`
- Exemplos: `llama2`, `codellama`, `mistral`, `deepseek-coder`

```json
{
  "cappy.chat.ollama.models": [
    "llama2",
    "codellama",
    "mistral",
    "deepseek-coder"
  ]
}
```

## ğŸ¨ Interface do UsuÃ¡rio

### Dropdown de Modelos
O dropdown exibe modelos agrupados por provedor com Ã­cones distintos:

- ğŸ§  **OpenAI** - GPT-4, GPT-3.5 Turbo
- ğŸ­ **Anthropic** - Claude 3 Opus, Claude 3 Sonnet
- â˜ï¸ **Azure** - Azure OpenAI Service
- ğŸ¦™ **Local** - Ollama, LM Studio, etc.

### Estados de Disponibilidade
- âœ… **DisponÃ­vel** - Modelo pronto para uso
- âš™ï¸ **Configure API Key** - Requer configuraÃ§Ã£o

## âš™ï¸ ConfiguraÃ§Ã£o

### Adicionar Modelo Personalizado

1. Abra as configuraÃ§Ãµes do VS Code (`Ctrl+,`)
2. Procure por "Cappy Chat"
3. Adicione um modelo em `Custom Models`:

```json
{
  "cappy.chat.customModels": [
    {
      "id": "claude-3-opus",
      "name": "Claude 3 Opus",
      "provider": "anthropic",
      "apiKey": "sk-ant-...",
      "available": true
    }
  ]
}
```

### Configurar Ollama

1. Instale Ollama: https://ollama.ai
2. Baixe modelos: `ollama pull llama2`
3. Configure no VS Code:

```json
{
  "cappy.chat.ollama.models": [
    "llama2",
    "codellama:13b",
    "mistral:7b-instruct"
  ]
}
```

## ğŸ”§ Arquitetura TÃ©cnica

### Fluxo de DetecÃ§Ã£o

```typescript
getAvailableModels()
  â”œâ”€ VS Code Language Model API
  â”‚  â””â”€ vscode.lm.selectChatModels()
  â”œâ”€ GitHub Copilot
  â”‚  â””â”€ vscode.extensions.getExtension('github.copilot')
  â”œâ”€ Custom Models
  â”‚  â””â”€ vscode.workspace.getConfiguration('cappy.chat')
  â””â”€ Ollama
     â””â”€ Check 'continue.continue' extension
```

### Provider Detection

```typescript
detectProvider(vendor: string): 'openai' | 'anthropic' | 'local' | 'azure'
```

Analisa metadata do modelo para determinar o provedor:
- `openai`, `gpt` â†’ OpenAI
- `anthropic`, `claude` â†’ Anthropic
- `azure` â†’ Azure
- Outros â†’ Local

## ğŸ“¡ ComunicaÃ§Ã£o Webview

### Mensagem: `modelsList`

Enviada quando o webview solicita modelos disponÃ­veis:

```typescript
{
  type: 'modelsList',
  models: [
    {
      id: 'gpt-4',
      name: 'GPT-4',
      provider: 'openai',
      available: true
    },
    // ...
  ]
}
```

### JavaScript: `updateModelsList(models)`

Atualiza o dropdown dinamicamente com os modelos recebidos:

```javascript
function updateModelsList(models) {
  const modelDropdown = document.getElementById('modelDropdown');
  modelDropdown.innerHTML = '';
  
  models.forEach(model => {
    // Criar opÃ§Ã£o para cada modelo
    const option = createModelOption(model);
    modelDropdown.appendChild(option);
  });
}
```

## ğŸ¯ Uso no Chat

1. **Abrir Cappy Chat**: Sidebar esquerda ou `Ctrl+Shift+P` â†’ "Cappy: Open Chat"
2. **Clicar no modelo atual** (ex: "GPT-4") para abrir dropdown
3. **Selecionar novo modelo** da lista
4. **Digitar prompt** e enviar
5. O agente selecionado processarÃ¡ a requisiÃ§Ã£o

## ğŸ”® Modelos Suportados

### OpenAI
- GPT-4, GPT-4 Turbo
- GPT-3.5 Turbo
- Custom deployments

### Anthropic
- Claude 3 Opus
- Claude 3 Sonnet
- Claude 3 Haiku

### Local (Ollama)
- Llama 2, 3
- CodeLlama
- Mistral
- DeepSeek Coder
- Phi-2, Phi-3

### Azure
- Azure OpenAI Service
- Custom endpoints

## ğŸ“Š Debug

Para ver os modelos detectados, abra o console do desenvolvedor:

```javascript
console.log('ğŸ¤– Available LLM models:', models);
```

Ou verifique os logs da extensÃ£o:
- `Ctrl+Shift+P` â†’ "Developer: Show Extension Host"
- Procure por logs `ChatProvider`

## ğŸš€ PrÃ³ximos Passos

- [ ] IntegraÃ§Ã£o real com APIs dos provedores
- [ ] Cache de respostas
- [ ] Streaming de respostas
- [ ] Multi-turn conversations com contexto
- [ ] Fine-tuning de modelos locais
- [ ] MÃ©tricas de custo por modelo

## ğŸ“ Exemplos de Uso

### Usar GPT-4 para explicaÃ§Ã£o de cÃ³digo
```
Modelo: GPT-4
Prompt: "Explain this React component"
```

### Usar Claude 3 para refatoraÃ§Ã£o
```
Modelo: Claude 3 Opus
Prompt: "Refactor this function to be more maintainable"
```

### Usar Llama local para testes
```
Modelo: Ollama: llama2
Prompt: "Generate unit tests for this class"
```

## ğŸ¦« Filosofia Cappy

> **Escolha seu agente, mantenha seu contexto**

O Cappy Chat permite que vocÃª escolha o melhor modelo para cada tarefa, mantendo todo o contexto e ferramentas do Cappy disponÃ­veis independente do modelo selecionado.
