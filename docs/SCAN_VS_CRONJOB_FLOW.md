# Scan vs CronJob Flow
## Como funciona o fluxo de processamento de arquivos no Cappy

**Data:** 30 de outubro de 2025

---

## üîÑ Arquitetura de Processamento em 2 Etapas

### Vis√£o Geral

O Cappy usa uma **arquitetura em 2 etapas** para processar arquivos:

1. **Scan (R√°pido)**: Descobre arquivos e salva metadados
2. **CronJob (Background)**: Processa arquivos e gera embeddings/vectors

Esta separa√ß√£o permite:
- ‚úÖ Scan r√°pido sem bloquear a UI
- ‚úÖ Processamento pesado em background
- ‚úÖ Feedback progressivo ao usu√°rio
- ‚úÖ Resili√™ncia a falhas (retry autom√°tico)

---

## üìä Fluxo Detalhado

### Etapa 1: Workspace Scan (Fast Discovery)

**Comando:** `Cappy: Scan Workspace`  
**Arquivo:** `src/nivel1/adapters/vscode/commands/scan-workspace.ts`  
**Worker:** `WorkspaceScanner` ‚Üí `FileMetadadoProcessor`

#### O que faz:

```
1. Descobrir arquivos no workspace
   ‚îî‚îÄ> FileDiscovery.discoverFiles()

2. Calcular hash dos arquivos (BLAKE3)
   ‚îî‚îÄ> Detectar mudan√ßas desde √∫ltimo scan

3. Filtrar arquivos que precisam processamento
   ‚îî‚îÄ> Apenas novos ou modificados

4. Salvar APENAS metadados no banco
   ‚îî‚îÄ> file_metadata.db
   ‚îî‚îÄ> Status: 'pending'
   ‚îî‚îÄ> Hash, path, timestamp
   ‚îî‚îÄ> SEM processar conte√∫do ainda!

5. Marcar arquivos como 'pending' na fila
   ‚îî‚îÄ> Prontos para processamento pelo CronJob
```

#### O que N√ÉO faz:
- ‚ùå N√ÉO gera embeddings
- ‚ùå N√ÉO cria vectors
- ‚ùå N√ÉO cria chunks no grafo
- ‚ùå N√ÉO processa AST
- ‚ùå N√ÉO analisa relacionamentos

**Resultado:** Lista de arquivos `pending` no banco de metadados

---

### Etapa 2: CronJob (Background Processing)

**Servi√ßo:** `FileProcessingCronJob`  
**Arquivo:** `src/nivel2/infrastructure/services/file-processing-cronjob.ts`  
**Worker:** `FileProcessingWorker` ‚Üí `IndexingService`

#### Quando inicia:
- ‚úÖ Automaticamente ao abrir workspace (se `autoStart: true`)
- ‚úÖ Intervalo configur√°vel (default: 10 segundos)
- ‚úÖ Processa um arquivo por vez (semaphore)

#### O que faz:

```
1. Buscar pr√≥ximo arquivo 'pending' no banco
   ‚îî‚îÄ> FileMetadataDatabase.getFilesByStatus('pending')

2. Atualizar status para 'processing'
   ‚îî‚îÄ> Evita processamento duplicado

3. Processar arquivo (FileProcessingWorker)
   ‚îú‚îÄ> Carregar conte√∫do
   ‚îú‚îÄ> Parsear e extrair chunks (ParserService)
   ‚îú‚îÄ> Indexar arquivo (IndexingService)
   ‚îÇ   ‚îú‚îÄ> Gerar embeddings (EmbeddingService)
   ‚îÇ   ‚îú‚îÄ> Criar file node no grafo
   ‚îÇ   ‚îú‚îÄ> Criar chunk nodes no grafo
   ‚îÇ   ‚îú‚îÄ> Descobrir entities (AST)
   ‚îÇ   ‚îú‚îÄ> Criar relacionamentos (CONTAINS, DOCUMENTS, etc)
   ‚îÇ   ‚îî‚îÄ> üéØ Inserir vectors (VectorStore.upsertChunks)
   ‚îÇ
   ‚îî‚îÄ> Retornar m√©tricas (chunks, nodes, relationships)

4. Atualizar status para 'processed'
   ‚îî‚îÄ> Marcar como completo com m√©tricas

5. Repetir para pr√≥ximo arquivo 'pending'
```

#### O que faz (Detalhes de IndexingService):

**Arquivo:** `src/nivel2/infrastructure/services/indexing-service.ts`

```typescript
async indexFile(filePath: string, language: string, chunks: DocumentChunk[]) {
  // 1. Gerar embeddings para todos os chunks
  const embeddings = await embeddingService.embedBatch(chunks);
  
  // 2. Criar file node no grafo
  await graphStore.createFileNode(filePath, language, linesOfCode);
  
  // 3. Criar chunk nodes no grafo
  await graphStore.createChunkNodes(chunks);
  
  // 4. Descobrir e resolver entities (AST analysis)
  await discoverAndResolveEntities(language, chunks);
  
  // 5. Criar relacionamentos CONTAINS (File -> Chunks)
  await graphStore.createRelationships(containsRels);
  
  // 6. Criar relacionamentos DOCUMENTS (JSDoc -> Code)
  await graphStore.createRelationships(documentsRels);
  
  // 7. Criar relacionamentos entre arquivos (imports, etc)
  await buildFileRelationshipsIncremental(filePath, chunks);
  
  // üéØ 8. INSERIR VECTORS COM EMBEDDINGS
  await vectorStore.upsertChunks(chunks);
}
```

**Resultado:** 
- ‚úÖ Chunks no grafo (tabela `nodes`)
- ‚úÖ Vectors no banco (tabela `vectors`)
- ‚úÖ Embeddings no vec_vectors (sqlite-vec)
- ‚úÖ Relacionamentos criados (tabela `edges`)

---

## üîç Problema Identificado: 839 Chunks Sem Vectors

### Causa Raiz

**Hip√≥tese:** Arquivos foram processados parcialmente ou o CronJob falhou

Poss√≠veis causas:
1. **CronJob n√£o estava rodando** quando scan foi feito
2. **Processo interrompido** antes de completar
3. **Erro no IndexingService** ao gerar embeddings
4. **VectorStore estava null** durante processamento
5. **Race condition** entre scan e cronjob

### Como Confirmar

```sql
-- Ver arquivos que est√£o 'pending' (n√£o processados ainda)
SELECT COUNT(*) FROM file_metadata WHERE status = 'pending';

-- Ver arquivos com erro
SELECT filePath, errorMessage, retryCount 
FROM file_metadata 
WHERE status = 'error';

-- Ver arquivos processados mas sem vectors
SELECT n.id, n.label 
FROM nodes n
WHERE n.type = 'chunk'
  AND NOT EXISTS (SELECT 1 FROM vectors WHERE chunk_id = n.id)
LIMIT 10;
```

### Solu√ß√£o Correta

**N√ÉO** rodar "Cappy: Scan Workspace" novamente!  
O scan s√≥ marca arquivos como `pending`, n√£o gera vectors.

**Op√ß√£o 1: Deixar CronJob processar** (recomendado)
```
1. Verificar se CronJob est√° rodando
2. Aguardar processamento autom√°tico
3. Monitorar progresso no painel
```

**Op√ß√£o 2: For√ßar reprocessamento**
```sql
-- Marcar chunks sem vectors como 'pending' novamente
UPDATE file_metadata 
SET status = 'pending', 
    retryCount = 0,
    errorMessage = NULL
WHERE id IN (
  SELECT fm.id FROM file_metadata fm
  INNER JOIN nodes n ON n.id LIKE fm.filePath || '%'
  WHERE n.type = 'chunk'
    AND NOT EXISTS (SELECT 1 FROM vectors WHERE chunk_id = n.id)
);
```

**Op√ß√£o 3: Reset e rescan completo** (√∫ltimo recurso)
```
1. "Cappy: Reset Graph Database"
2. "Cappy: Scan Workspace"
3. Aguardar CronJob processar tudo
```

---

## üìä Monitoramento

### Como verificar se CronJob est√° rodando

```typescript
// No c√≥digo da extens√£o
const cronJob = context.globalState.get('fileProcessingCronJob');
console.log('CronJob running:', cronJob?.isRunning());
```

### Como ver progresso

```sql
-- Ver arquivos por status
SELECT status, COUNT(*) 
FROM file_metadata 
GROUP BY status;

-- Ver arquivos sendo processados
SELECT filePath, currentStep, progress 
FROM file_metadata 
WHERE status = 'processing';

-- Ver √∫ltimos processados
SELECT filePath, processingCompletedAt, chunksCount, nodesCount
FROM file_metadata 
WHERE status = 'processed'
ORDER BY processingCompletedAt DESC
LIMIT 10;
```

---

## üéØ Resumo para o Dev

### O que o Scan faz:
```
Scan = Descoberta r√°pida + Metadados + Fila de 'pending'
```

### O que o CronJob faz:
```
CronJob = Processar 'pending' ‚Üí Parse ‚Üí Embeddings ‚Üí Vectors ‚Üí Grafo
```

### Onde vectors s√£o criados:
```
IndexingService.indexFile() 
  ‚îî‚îÄ> vectorStore.upsertChunks(chunks) 
      ‚îî‚îÄ> Insere na tabela 'vectors' com embeddings
```

### Por que 839 chunks n√£o t√™m vectors:
```
Poss√≠vel causa:
1. Scan criou chunks no grafo
2. CronJob n√£o processou (parou, erro, n√£o rodando)
3. Resultado: Chunks existem mas sem vectors
```

### Solu√ß√£o:
```
1. Verificar CronJob est√° rodando
2. Verificar logs de erro no file_metadata
3. Reprocessar arquivos 'pending' ou com erro
4. N√ÉO rodar scan novamente (n√£o resolve)
```

---

## üîß Comandos √öteis

### Verificar estado atual
```bash
# Contar arquivos por status
sqlite3 .cappy/data/file-metadata.db \
  "SELECT status, COUNT(*) FROM file_metadata GROUP BY status;"

# Ver chunks sem vectors
sqlite3 .cappy/data/graph-store.db \
  "SELECT COUNT(*) FROM nodes n WHERE n.type = 'chunk' 
   AND NOT EXISTS (SELECT 1 FROM vectors WHERE chunk_id = n.id);"
```

### For√ßar reprocessamento de arquivos espec√≠ficos
```sql
UPDATE file_metadata 
SET status = 'pending' 
WHERE filePath IN ('src/path/to/file.ts');
```

### Ver logs do CronJob
```
# No VS Code Developer Tools Console
# Filtrar por [CronJob]
```

---

**Conclus√£o:** O scan √© r√°pido porque **n√£o processa** os arquivos, apenas os **descobre e marca como pending**. O processamento pesado (embeddings, vectors, AST, relacionamentos) acontece no **CronJob em background**.

Os 839 chunks sem vectors provavelmente foram criados durante um scan, mas o CronJob n√£o os processou ainda ou falhou ao processar.
